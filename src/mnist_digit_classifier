import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

np.random.seed(45)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))


# Activation functions 
def relu(z):
    return np.maximum(0, z)

def relu_gradient(z):
    return (z > 0).astype(float)

def activation(z):
    return sigmoid(z)
    
def activation_derivative(z):
    return sigmoid_derivative(z)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))
    return exp_z / np.sum(exp_z, axis=0, keepdims=True)

def softmax_derivative(z):
    return softmax(z) * (1 - softmax(z))

# Initialization 
def randInitializeWeights(layers):
    W = {}
    b = {}
    for i in range(1, len(layers)):
        
        #epsilon_init = (math.sqrt(6))/(math.sqrt(layers[i-1] + layers[i]))
      
        #W[i] = np.random.randn(layers[i], layers[i-1]) * 2 * epsilon_init - epsilon_init
        #b[i] = np.zeros((layers[i], 1)) 
        
        
        #He initialization
        epsilon_init = (math.sqrt(2))/layers[i-1]
        W[i] = np.random.randn(layers[i], layers[i-1]) * epsilon_init 
        b[i] = np.zeros((layers[i], 1))
        
        
    return W, b

# Forward propagation 
def forward_prop(W, a, b, layers, z):
    for i in range(1, len(layers)):
        z[i] = np.matmul(W[i], a[i-1]) + b[i]
        #a[i] = activation(z[i])
        if i < len(layers)-1:
            a[i] = relu(z[i])
        else:
            a[i] = softmax(z[i])
    return z, a

# Cost function 
def nnCostFunction(y, y_hat, W, lambda_):
    m = y.shape[1]
    J = (1/m) * np.sum(-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)) + (lambda_ / (2 * m)) * sum(np.sum(W[k]**2) for k in W)
    return J

# Gradient 
def cost_gradient(y, y_hat):
    dC = ((y_hat - y)/(y_hat*(1-y_hat)))
    return dC

# Backpropagation 
def backward_propagation(a, y, W, b, z, layers, m):
    dW = {}
    db = {}
    dz = {}
    L = len(layers) - 1

    #dz[L] = cost_gradient(y, a[L]) * sigmoid_derivative(z[L])
    dz[L] = cost_gradient(y, a[L]) * softmax_derivative(z[L])
    dW[L] = (1/m) * (np.matmul(dz[L], a[L-1].T))
    db[L] = (1/m) * (np.sum(dz[L], axis=1, keepdims=True))

    for l in range(L-1, 0, -1):
        dz[l] = np.matmul(W[l+1].T, dz[l+1]) * relu_gradient(z[l])
        dW[l] = (1/m) * np.matmul(dz[l], a[l-1].T)
        db[l] = (1/m) * np.sum(dz[l], axis=1, keepdims=True)  
  
    return dW, db

def gradient_check(x, y, W, b, layers,lambda_):
    # Use a small subset of data
    num_samples = 50
    x_small = x[:, :num_samples]
    y_small = y[:, :num_samples]
    a = {0: x_small}
    m = x_small.shape[1]
    
    epsilon = 1e-4
    z = {}
    z, a = forward_prop(W, a, b, layers, z)
    dW, _ = backward_propagation(a, y_small, W, b, z, layers, m)
    
    check_layer = 1
    numerical_grad_W = np.zeros_like(W[check_layer])
    W_original = W[check_layer].copy()
    
    # Check a subset of weights
    num_checks = 350
    indices = np.random.choice(W[check_layer].size, num_checks, replace=False)
    rows, cols = np.unravel_index(indices, W[check_layer].shape)
    
    for idx in range(num_checks):
        i, j = rows[idx], cols[idx]
        W[check_layer][i, j] = W_original[i, j] + epsilon
        z_plus, a_plus = forward_prop(W, {0: x_small}, b, layers, {})
        cost_plus = nnCostFunction(y_small, a_plus[len(layers)-1], W, lambda_)
        
        W[check_layer][i, j] = W_original[i, j] - epsilon
        z_minus, a_minus = forward_prop(W, {0: x_small}, b, layers, {})
        cost_minus = nnCostFunction(y_small, a_minus[len(layers)-1], W, lambda_)
        
        W[check_layer][i, j] = W_original[i, j]
        numerical_grad_W[i, j] = (cost_plus - cost_minus) / (2 * epsilon)
    
    numerator = np.linalg.norm(numerical_grad_W - dW[check_layer])
    denominator = np.linalg.norm(numerical_grad_W) + np.linalg.norm(dW[check_layer])
    difference = numerator / denominator if denominator != 0 else numerator
    
    print(f"\nGradient difference: {difference}")
    return dW, numerical_grad_W


def visualize_weights(W, layer_idx=1, layer_size=None, img_shape=(28, 28)):
    # Set layer_size to the number of neurons in the layer if not specified
    layer_size = min(layer_size, W[layer_idx].shape[0]) if layer_size else W[layer_idx].shape[0]
    
    # Calculate grid size using np.ceil
    cols = int(np.ceil(np.sqrt(layer_size)))  
    rows = int(np.ceil(layer_size / cols))    
    
    plt.figure(figsize=(10, 10))
    for i in range(layer_size):
        # Reshape weights to image shape (28x28 for MNIST)
        weight_img = W[layer_idx][i, :].reshape(img_shape)
        
        plt.subplot(rows, cols, i + 1)
        plt.imshow(weight_img, cmap='gray')
        plt.title(f'Neuron {i + 1}', fontsize=8)
        plt.axis('off')
    
    plt.suptitle(f'Weights Visualization for Layer {layer_idx}', fontsize=14)
    plt.tight_layout(rect=[0, 0, 1, 0.95])  
    plt.show()

# Visualize activations of hidden layers for a single sample
def visualize_activations(W, b, x_sample, layers, sample_idx=0):
    # Forward propagate a single sample
    a = {0: x_sample[:, sample_idx].reshape(-1, 1)}  # Shape: (784, 1)
    z = {}
    
    for i in range(1, len(layers)-1):
        z[i] = np.matmul(W[i], a[i-1]) + b[i]
        a[i] = relu(z[i])  # Use ReLU for hidden layers
    
    z[len(layers)-1] = np.matmul(W[len(layers)-1], a[len(layers)-2]) + b[len(layers)-1]
    a[len(layers)-1] = softmax(z[len(layers)-1])
    
    # Plot original image and activations
    plt.figure(figsize=(15, 4))
  

    
    plt.tight_layout()
    plt.show()
    


# Helper functions 
def encode_labels(y, num_classes):
    encoded = np.zeros((num_classes, len(y)))
    for i in range(len(y)):
        encoded[y[i], i] = 1
    return encoded

def predict(W, b, x, layers):
    a = {0: x}
    z = {}
    for i in range(1, len(layers)):
        z[i] = np.matmul(W[i], a[i-1]) + b[i]
        a[i] = relu(z[i]) if i < len(layers)-1 else softmax(z[i])
    return np.argmax(a[len(layers)-1], axis=0)


# Data loading 
df_train = pd.read_csv("mnist_train.csv")
df_test = pd.read_csv("mnist_test.csv")

df_sample = df_train.iloc[:5000]
df_test_sample = df_test.iloc[:1000]

x_train = df_sample.drop("label", axis=1).values.T / 255
y_train = df_sample["label"].values
x_test = df_test_sample.drop("label", axis=1).values.T / 255
y_test = df_test_sample["label"].values

# Network setup (unchanged)
layers = [784, 25, 25, 10]
m_train = x_train.shape[1]
m_test = x_test.shape[1]

# Initialize weights and biases
W, b = randInitializeWeights(layers)



# Encode labels
y_train_encoded = encode_labels(y_train, layers[-1])
y_test_encoded = encode_labels(y_test, layers[-1])



# Training parameters
max_iter = 5000
alpha = 0.01
lambda_ = 100
train_cost_history = []
test_cost_history = []  # Added for test cost tracking

# Gradient checking
dW_analytical, grad_approx = gradient_check(x_train[:, :10], y_train_encoded[:, :10], W, b, layers, lambda_)

for iteration in range(max_iter):
    # Training forward pass
    z_train = {}
    z_train, a_train = forward_prop(W, {0: x_train}, b, layers, z_train)
    
    # Backpropagation
    dW, db = backward_propagation(a_train, y_train_encoded, W, b, z_train, layers, m_train)
    
    # Update parameters
    for l in range(1, len(layers)):
        W[l] = W[l] - alpha * (dW[l] + (lambda_/m_train) * W[l])
        b[l] = b[l] - alpha * db[l]
    
    # Calculate training cost
    train_cost = nnCostFunction(y_train_encoded, a_train[len(layers)-1], W, lambda_)
    train_cost_history.append(train_cost)
    
    # Calculate test cost 
    z_test = {}
    z_test, a_test = forward_prop(W, {0: x_test}, b, layers, z_test)
    test_cost = nnCostFunction(y_test_encoded, a_test[len(layers)-1], W, lambda_)
    test_cost_history.append(test_cost)
    
    if iteration % 300 == 0:
        print(f"Epoch: {iteration}, Train Cost: {train_cost:.4f}, Test Cost: {test_cost:.4f}")

# Evaluation
train_pred = predict(W, b, x_train, layers)
train_acc = np.mean(train_pred == y_train) * 100
test_pred = predict(W, b, x_test, layers)
test_acc = np.mean(test_pred == y_test) * 100

print(f"\nTraining Accuracy: {train_acc:.2f}%")
print(f"Test Accuracy: {test_acc:.2f}%")



# Plotting - Two side-by-side plots
plt.figure(figsize=(14, 5))  # Wider figure to accommodate two plots

# Training Cost Plot (Left)
plt.subplot(1, 2, 1)  # 1 row, 2 cols, first plot
plt.plot(train_cost_history, 'b-', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Cost', fontsize=12)
plt.title('Training Cost Over Epochs', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)
plt.xlim(0, len(train_cost_history))

# Test Cost Plot (Right)
plt.subplot(1, 2, 2)  # 1 row, 2 cols, second plot
plt.plot(test_cost_history, 'r-', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Cost', fontsize=12)
plt.title('Test Cost Over Epochs', fontsize=14)
plt.grid(True, linestyle='--', alpha=0.7)
plt.xlim(0, len(test_cost_history))

plt.tight_layout(pad=3.0)  # Adds spacing between plots
plt.show()

#visualization
visualize_weights(W, layer_idx=1, layer_size=25, img_shape=(28, 28))
visualize_activations(W, b, x_test, layers, sample_idx=0)
